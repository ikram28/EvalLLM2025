{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ========== INSTALL DEPENDENCIES ==========\n"
      ],
      "metadata": {
        "id": "RdkQk-rizwQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "PHWWZT5O1d1a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --upgrade"
      ],
      "metadata": {
        "id": "vbMaqBap5iJn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== IMPORT LIBRARIES ==========\n"
      ],
      "metadata": {
        "id": "2FpKIT-yhKBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import ast, pprint\n"
      ],
      "metadata": {
        "id": "0QOpDxt5hL6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "\n"
      ],
      "metadata": {
        "id": "3jcnAZQYhMj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=\"YOUR-API-KEY\"\n",
        ")\n",
        "EMBED_MODEL = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "CHAT_MODEL  = 'gpt-4.1'\n",
        "K           = 10\n"
      ],
      "metadata": {
        "id": "QDojyRYThYX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== SYSTEM PROMPT FOR EVENT EXTRACTION ==========\n"
      ],
      "metadata": {
        "id": "VBh462xWhbfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT =(\"\"\"You are an epidemiology analyst. Your job is to extract structured events from French articles.\n",
        "\n",
        "═════════ TASK ═════════\n",
        "INPUT:\n",
        "• A French article.\n",
        "• A list of extracted named entities (ID, text span, and type).\n",
        "\n",
        "OUTPUT:\n",
        "Return a JSON array named events following this schema:\n",
        "\n",
        "[\n",
        "  [\n",
        "    {\"attribute\":\"evt:central_element\", \"occurrences\":[\"ID_c1\", \"ID_c2\", ...]},\n",
        "    {\"attribute\":\"evt:associated_element\", \"occurrences\":[\"ID_a1\", \"ID_a2\", ...]}\n",
        "  ],\n",
        "]\n",
        "\n",
        "═════════ RULES ═════════\n",
        " 1. CENTRAL ELEMENT — REQUIRED (1 per event)\n",
        "- Must be exactly one of: INF_DISEASE, NON_INF_DISEASE, PATHOGEN, DIS_REF_TO_PATH, PATH_REF_TO_DIS, RADIOISOTOPE, TOXIC_C_AGENT, EXPLOSIVE, BIO_TOXIN\n",
        "- Each event has exactly one central element (but it may have several synonymous IDs see Rule 4).\n",
        "\n",
        " 2. ASSOCIATED ELEMENTS — REQUIRED (at least one location + at least one date/periode)\n",
        "Add all entity IDs relevant to:\n",
        "- Locations: LOCATION, LOC_REF_TO_ORG, ORG_REF_TO_LOC\n",
        "- Dates: ABS_DATE, REL_DATE, ABS_PERIOD, REL_PERIOD, FUZZY_PERIOD, DOC_DATE\n",
        "- Use DOC_DATE only if no other date is found.\n",
        "- Prefer absolute over relative dates if both exist.\n",
        "\n",
        "3. WINDOW OF RELEVANCE\n",
        "- Start from the sentence containing the central element.\n",
        "- If no associated location/date is there, check the adjacent sentences.\n",
        "\n",
        "4. SYNONYMS\n",
        "If several entity IDs refer to the same real‑world object (e.g. three mentions of “uranium 238”, or “Paris” vs “Ville‑Lumière”, or different surface forms of the same date),  include all those IDs together in the same occurrences list.\n",
        "\n",
        "5. EVENT LIMIT\n",
        "- Max 10 events.\n",
        "- If more are present, keep the 10 most relevant to public health risk.\n",
        "\n",
        "6. VALIDITY\n",
        "- Each entity ID appears in only one event.\n",
        "- Output must be valid JSON and contain nothing else.\n",
        "\n",
        "═════════ TIPS ═════════\n",
        " For event splitting, use this rule:\n",
        "– Same central + coherent dates/places → merge into one event.\n",
        "– Distant in time/space or different causes → separate events.\n",
        "\n",
        "When in doubt between including or skipping an associated element: include it if it helps answer: Where? When? What agent?\n",
        "\n",
        "══════════ EXAMPLES ══════════\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "_dpUL0C50ehw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== FILE PATHS ==========\n"
      ],
      "metadata": {
        "id": "ukCB9sIPheSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_JSON = \"the train file for few shot examples\"\n",
        "TEST_JSON  = \"Your input file containing already extracted entities into the challenge format\""
      ],
      "metadata": {
        "id": "rtYUWuSqhpxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== LOAD AND FILTER TRAIN DATA ==========\n"
      ],
      "metadata": {
        "id": "JiYIQOFMhuba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(TRAIN_JSON, encoding=\"utf-8\") as f:\n",
        "    train_docs = json.load(f)\n",
        "\n",
        "train_texts        = [d[\"text\"]     for d in train_docs]\n",
        "train_entities     = [d[\"entities\"] for d in train_docs]\n",
        "train_events       = [d[\"events\"]   for d in train_docs]\n",
        "\n",
        "# Filter out training docs without annotated events\n",
        "valid_train_idxs   = [i for i, e in enumerate(train_events) if e]\n",
        "valid_train_texts  = [train_texts[i]     for i in valid_train_idxs]\n",
        "valid_train_entities = [train_entities[i] for i in valid_train_idxs]\n",
        "valid_train_events   = [train_events[i]   for i in valid_train_idxs]\n",
        "\n",
        "# ========== COMPUTE TRAIN EMBEDDINGS ==========\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "train_embeddings = embedder.encode(valid_train_texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# ========== LOAD TEST DATA ==========\n",
        "with open(TEST_JSON, encoding=\"utf-8\") as f:\n",
        "    test_docs = json.load(f)\n",
        "\n",
        "test_texts    = [d[\"text\"]     for d in test_docs]\n",
        "test_entities = [d[\"entities\"] for d in test_docs]\n"
      ],
      "metadata": {
        "id": "OoItkLzsynHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== BUILD OPENAI BATCH TASKS ==========\n"
      ],
      "metadata": {
        "id": "raqkOy7iiQtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = []\n",
        "\n",
        "for i, (text, entities) in enumerate(zip(test_texts, test_entities)):\n",
        "    # Encode test text\n",
        "    test_embedding = embedder.encode([text], convert_to_numpy=True)[0]\n",
        "\n",
        "    # Compute similarity with train embeddings\n",
        "    sims = cosine_similarity([test_embedding], train_embeddings)[0]\n",
        "\n",
        "    # Get top-k similar train documents\n",
        "    k = min(K, len(sims))\n",
        "    best_pos = np.argpartition(sims, -k)[-k:]\n",
        "    best_pos = best_pos[np.argsort(sims[best_pos])[::-1]]\n",
        "\n",
        "    # Build few-shot prompt blocks\n",
        "    few_shot_blocks = [\n",
        "        \"INPUT: {}\\nENTITIES: {}\\nOUTPUT: {}\\n\".format(\n",
        "            valid_train_texts[pos],\n",
        "            json.dumps(valid_train_entities[pos], ensure_ascii=False),\n",
        "            json.dumps(valid_train_events[pos],   ensure_ascii=False)\n",
        "        )\n",
        "        for pos in best_pos if sims[pos] > -1\n",
        "    ]\n",
        "\n",
        "    prompt_with_few_shot = SYSTEM_PROMPT\n",
        "    if few_shot_blocks:\n",
        "        prompt_with_few_shot += \"\\n\\n\" + \"\\n\\n\".join(few_shot_blocks)\n",
        "\n",
        "    user_content = f\"INPUT: {text}\\nENTITIES: {json.dumps(entities, ensure_ascii=False)}\\nOUTPUT: \"\n",
        "\n",
        "    # Create a task entry for the batch\n",
        "    tasks.append({\n",
        "        \"custom_id\": f\"task-{i}\",\n",
        "        \"method\": \"POST\",\n",
        "        \"url\": \"/v1/chat/completions\",\n",
        "        \"body\": {\n",
        "            \"model\": CHAT_MODEL,\n",
        "            \"temperature\": 0,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": prompt_with_few_shot},\n",
        "                {\"role\": \"user\",   \"content\": user_content}\n",
        "            ]\n",
        "        }\n",
        "    })\n",
        "\n",
        "print(f\"Built {len(tasks)} tasks using top-{K} valid train docs per test entry.\")"
      ],
      "metadata": {
        "id": "ETz7go0ciRLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== SAVE BATCH TASKS TO JSONL ==========\n"
      ],
      "metadata": {
        "id": "QZXwM7PKiVNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"batch_evalLLM.jsonl\"\n",
        "\n",
        "with open(file_name, 'w') as file:\n",
        "    for obj in tasks:\n",
        "        file.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n"
      ],
      "metadata": {
        "id": "gXkB1_HiBwAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== SUBMIT BATCH TO OPENAI API ==========\n"
      ],
      "metadata": {
        "id": "nyJNyn5giZ5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_file = client.files.create(\n",
        "  file=open(file_name, \"rb\"),\n",
        "  purpose=\"batch\"\n",
        ")"
      ],
      "metadata": {
        "id": "irCuXb0DB0h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_job = client.batches.create(\n",
        "  input_file_id=batch_file.id,\n",
        "  endpoint=\"/v1/chat/completions\",\n",
        "  completion_window=\"24h\"\n",
        ")"
      ],
      "metadata": {
        "id": "TdQD5IDnB6o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_job = client.batches.retrieve(batch_job.id)\n",
        "print(batch_job)"
      ],
      "metadata": {
        "id": "nwCeVaXyB8Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_job.status"
      ],
      "metadata": {
        "id": "HI-2lGyECAHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== RETRIEVE BATCH OUTPUT ==========\n"
      ],
      "metadata": {
        "id": "5Lv-kkK_ie6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_file_id = batch_job.output_file_id\n",
        "result = client.files.content(result_file_id).content"
      ],
      "metadata": {
        "id": "5MKnicdqCBy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result_file_name = \"batch_job_results_evalLLM.jsonl\"\n",
        "\n",
        "with open(result_file_name, 'wb') as file:\n",
        "    file.write(result)"
      ],
      "metadata": {
        "id": "AE9etlyyCCsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== PARSE RESULTS AND BUILD FINAL OUTPUT ==========\n"
      ],
      "metadata": {
        "id": "UdV2DZEMiijT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "with open(result_file_name, 'r') as file:\n",
        "    for line in file:\n",
        "        json_object = json.loads(line.strip())\n",
        "        results.append(json_object)"
      ],
      "metadata": {
        "id": "QJ_3UotwC0HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for res in results:\n",
        "    cid = res[\"custom_id\"]\n",
        "    idx = int(cid.split(\"-\")[1])\n",
        "    result = res['response']['body']['choices'][0]['message']['content']\n",
        "    item=test_docs[idx]\n",
        "    l.append({\n",
        "        \"text\": item['text'],\n",
        "        \"entities\": item['entities'],\n",
        "        \"events\":  result\n",
        "    })\n",
        "\n",
        "with open('OUTPUT-FILE', 'w', encoding='utf-8') as f:\n",
        "    json.dump(l, f, ensure_ascii=False, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "wovMBhrNBs6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========== CLEANING & STRUCTURING OUTPUT EVENTS ==========\n"
      ],
      "metadata": {
        "id": "k1f_muW9ipK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAW_JSON   = \"OUTPUT-FILE\"\n",
        "CLEAN_JSON = \"OUTPUT-FILE-cleaned.json\"\n",
        "\n",
        "# Convert an 'events' field to a valid list, handling strings, dicts, and malformed cases.\n",
        "def parse_events_field(events_field):\n",
        "\n",
        "    if isinstance(events_field, list):\n",
        "        return events_field\n",
        "\n",
        "    if isinstance(events_field, dict) and \"events\" in events_field:\n",
        "        return events_field[\"events\"]\n",
        "\n",
        "    if isinstance(events_field, str):\n",
        "        text = events_field.strip()\n",
        "        if not text:\n",
        "            return []\n",
        "        try:\n",
        "            obj = json.loads(text)\n",
        "            return obj[\"events\"] if isinstance(obj, dict) and \"events\" in obj else obj\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "        try:\n",
        "            obj = ast.literal_eval(text)\n",
        "            return obj[\"events\"] if isinstance(obj, dict) and \"events\" in obj else obj\n",
        "        except Exception:\n",
        "            print(\"⚠️  could not parse one events string → left empty\")\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "# ---------------- load, convert, save -------------------------------\n",
        "with open(RAW_JSON, encoding=\"utf-8\") as f:\n",
        "    docs = json.load(f)\n",
        "\n",
        "for doc in docs:\n",
        "    doc[\"events\"] = parse_events_field(doc.get(\"events\", []))\n",
        "\n",
        "with open(CLEAN_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(docs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Converted strings → lists with NO structural changes.\")\n",
        "print(\"Clean file saved to:\", CLEAN_JSON)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "llqRd1N8oSiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
